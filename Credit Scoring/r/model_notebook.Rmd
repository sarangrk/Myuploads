---
title: "Modeling outside Shiny"
output: html_notebook
---

```{r,echo=FALSE}
## Load required libraries
library(dplyr)
library(ggplot2)
library(RODBC)
library(glue)

library(caret)
library(pROC)
library(ROCR)
```

Before the modeling process, we have to load the required tables and perform the transformation from the WOE table. This step requires connecting to Teradata (It is important to install the Teradata Driver and name it TeradataDriver)
```{r}
coarse_class_tbl <- "JAPAN_DAP.TBL_COARSE_CLASSES_tmp"
coarse_class_woe_tbl <- "JAPAN_DAP.TBL_COARSE_CLASSES_WOE_tmp"
hierarchy_tbl <- "JAPAN_DAP.TBL_VARIABLE_HIERARCHY_tmp"
scorecard_tbl <- "JAPAN_DAP.TBL_SCORECARD_tmp"

charts_woe_savedir <- "output_charts/woe"
charts_md_savedir <- "output_charts/model_diagnostics"

con <- odbcDriverConnect(connection="Driver={TeradataDriver};DBCName=153.65.122.42;Database=dbc;Uid=dbc;Pwd=dbc;")
coarse_class_dt <- sqlQuery(con,glue("SELECT * FROM {coarse_class_tbl};"))
coarse_class_woe_dt <- sqlQuery(con,glue("SELECT * FROM {coarse_class_woe_tbl};"))
hierarchy_dt <- sqlQuery(con,glue("SELECT * FROM {hierarchy_tbl};"))
close(con)

train_dt <- read.csv("data/training_data.csv")
test_dt <- read.csv("data/test_data.csv")
    
incl_vars <- coarse_class_dt %>% select(VARIABLE) %>% pull %>% unique %>% as.character
```

```{r,message=FALSE,echo=FALSE}
convertwoetab <- function(bin_sample_dt){
      
      bin_sample_dt <- bin_sample_dt %>% select(ACCT_ID,UQ(incl_vars),TARGET)  
      for (curr_var in incl_vars){
        
        curr_var_lab <- paste0(curr_var,"_woe")
        
        bin_sample_dt <- bin_sample_dt %>% 
          rename(curr_var = UQ(curr_var)) %>% 
          inner_join(coarse_class_dt %>% 
                       filter(VARIABLE == curr_var) %>% 
                       inner_join(coarse_class_woe_dt %>% 
                                    select(VARIABLE,NEW_BIN,woe)),
                     by = c("curr_var" = "BIN")) %>% 
          group_by(ACCT_ID) %>% 
          mutate(!!curr_var_lab:=woe) %>% 
          select(-one_of(c("curr_var","VARIABLE","NEW_BIN","woe")))
        
        
      }
      
      return(bin_sample_dt %>%group_by(ACCT_ID) %>% select(matches("(_woe)$|(TARGET)")) %>% ungroup)
}

woe_train <- convertwoetab(train_dt)
woe_test <- convertwoetab(test_dt)
```

One possible model is a forward selection model based on groups of variables possibly added based on tiers. Because this model is not in any known R package, we create a custom function for it, *hierarchical_forward*.
```{r}
hierarchical_forward <- function(dat, target, list_hierarchy, 
                                   type="glm", family = "binomial", link = "logit"){
    
    prev_form <- formula(eval(paste0(target,"~1")))
    prev_form_txt <- paste0(target,"~1")
    prev_mod <- eval(parse(text=paste0(
      type,"(", prev_form_txt,", data = ","dat",", family = ",family,"(link = ",link,"))")))
    
    for (i in list_hierarchy){
      next_form <- update(prev_form,eval(paste0("~ . +",paste(i,collapse="+"))))
      
      upper_mod <- eval(parse(text=paste0(type,"(","next_form",",data=dat, family = ",family,"(link = ",link,"))")))
      
      next_mod <- step(prev_mod,scope=list(lower=prev_mod,upper=upper_mod),direction = "forward")
      
      
      prev_form <- next_mod$formula
      prev_mod <- next_mod
    }
    return(next_mod)
  }
```

I also repurposed a provided modeling code for a GLM and a stepwise logistic regression model in this process. However, I replaced the stepwise model with the hierarchical forward model shown above.
```{r}
target <- "TARGET"
train <- woe_train
test <- woe_test

res <- list()

woe_tbl <- coarse_class_woe_dt %>% select(VARIABLE) %>% unique

list_h <- hierarchy_dt %>% 
  semi_join(woe_tbl) %>%
  mutate(VARIABLE = paste0(VARIABLE,"_woe")) 
list_hierarchy <- split(list_h$VARIABLE, list_h$HIERARCHY)

pred_vars_all <- train %>% select(ends_with("_woe")) %>% colnames

dat <- train %>% select(target, pred_vars_all)

print("Creating full model")
res$glm     <- eval(parse(text = paste0(
  "glm(", target, " ~ ., data = dat, family = binomial(link = logit))")))

print("Creating hierarchical forward model")
res$glm_s   <- hierarchical_forward(dat=train,target,list_hierarchy)
# res$glm_s   <- step(res$glm, direction = "forward")

res$score   <- fitted(res$glm, type = "response")
res$score_s <- fitted(res$glm_s, type = "response")

res$score_test   <- predict(res$glm, newdata = test, type = "response")
res$score_s_test <- predict(res$glm_s, newdata = test, type = "response")

res$roc     <- roc(train[[target]], res$score)
res$roc_s   <- roc(train[[target]], res$score_s)


res$roc_test     <- roc(test[[target]], res$score_test)
res$roc_s_test   <- roc(test[[target]], res$score_s_test)

stat_vals <- c(
  coords(res$roc, x = "best",
         ret = c("threshold", "sensitivity", "specificity", "ppv", "npv"),
         best.method = "closest.topleft"))


stat_vals_s <- c(
  pROC::coords(res$roc_s, x = "best",
         ret = c("threshold", "sensitivity", "specificity", "ppv", "npv"),
         best.method = "closest.topleft"))

if(stat_vals_s[[1]] == -Inf){
  
  print("Forcing threshold to be 0.5")
  
  stat_vals_s <- c(
    pROC::coords(res$roc_s, x = 0.5, input = "threshold",
           ret = c("threshold", "sensitivity", "specificity", "ppv", "npv")
           # , best.method = "closest.topleft"
           ))
}

stat_vals_test <- c(
  coords(res$roc_test, x = stat_vals[[1]], input = "threshold",
         ret = c("threshold", "sensitivity", "specificity", "ppv", "npv")))

stat_vals_s_test <- c(
  coords(res$roc_s_test, x = stat_vals_s[[1]], input = "threshold",
         ret = c("threshold", "sensitivity", "specificity", "ppv", "npv")))

pred   <- prediction(res$score, train[[target]])
pred_s <- prediction(res$score_s, train[[target]])
perf   <- performance(pred, "tpr", "fpr")
perf_s <- performance(pred_s, "tpr", "fpr")
auc_value   <- abs(res$roc$auc)
auc_value_s <- abs(res$roc_s$auc)
ks_value    <- max(attr(perf, 'y.values')[[1]] - attr(perf, 'x.values')[[1]])
ks_value_s  <- max(attr(perf_s, 'y.values')[[1]] - attr(perf_s, 'x.values')[[1]])
F_value     <- 2 * stat_vals[2] * stat_vals[4] / (stat_vals[2] + stat_vals[4])
F_value_s   <- 2 * stat_vals_s[2] * stat_vals_s[4] / (stat_vals_s[2] + stat_vals_s[4])

res$class   <- if_else(res$score > stat_vals[1], 1, 0)
res$class_s <- if_else(res$score_s > stat_vals_s[1], 1, 0)

res$stat_vals   <- c(stat_vals, auc_value, ks_value, F_value, res$glm$aic)
res$stat_vals_s <- c(stat_vals_s, auc_value_s, ks_value_s, F_value_s, res$glm_s$aic)
names(res$stat_vals)[6:9]   <- c("AUC", "KS_value", "F_value", "AIC")
names(res$stat_vals_s)[6:9] <- c("AUC", "KS_value", "F_value", "AIC")
res$confusion_matrix   <- confusionMatrix(factor(res$class), factor(train[[target]]))
res$confusion_matrix_s <- confusionMatrix(factor(res$class_s), factor(train[[target]]))

# res$confusion_matrix   <- as.matrix(table(factor(res$class), factor(train[[target]])))
# res$confusion_matrix_s <- as.matrix(table(factor(res$class_s), factor(train[[target]])))

res$pred <- pred
res$pred_s <- pred_s




pred_test   <- prediction(res$score_test, test[[target]])
pred_s_test <- prediction(res$score_s_test, test[[target]])
perf_test   <- performance(pred_test, "tpr", "fpr")
perf_s_test <- performance(pred_s_test, "tpr", "fpr")
auc_value_test   <- abs(res$roc_test$auc)
auc_value_s_test <- abs(res$roc_s_test$auc)
ks_value_test    <- max(attr(perf_test, 'y.values')[[1]] - attr(perf_test, 'x.values')[[1]])
ks_value_s_test  <- max(attr(perf_s_test, 'y.values')[[1]] - attr(perf_s_test, 'x.values')[[1]])
F_value_test     <- 2 * stat_vals_test[2] * stat_vals_test[4] / (stat_vals_test[2] + stat_vals_test[4])
F_value_s_test   <- 2 * stat_vals_s_test[2] * stat_vals_s_test[4] / (stat_vals_s_test[2] + stat_vals_s_test[4])

res$class_test   <- if_else(res$score_test > stat_vals_test[1], 1, 0)
res$class_s_test <- if_else(res$score_s_test > stat_vals_s_test[1], 1, 0)

res$stat_vals_test   <- c(stat_vals_test, auc_value_test, ks_value_test, F_value_test, res$glm$aic)
res$stat_vals_s_test <- c(stat_vals_s_test, auc_value_s_test, ks_value_s_test, F_value_s_test, res$glm_s$aic)
names(res$stat_vals_test)[6:9]   <- c("AUC", "KS_value", "F_value", "AIC")
names(res$stat_vals_s_test)[6:9] <- c("AUC", "KS_value", "F_value", "AIC")
res$confusion_matrix_test   <- confusionMatrix(factor(res$class_test), factor(test[[target]]))
res$confusion_matrix_s_test <- confusionMatrix(factor(res$class_s_test), factor(test[[target]]))


res$pred_test <- pred_test
res$pred_s_test <- pred_s_test
```

Model Statistics in training data
```{r,echo=FALSE}
stat_vals <- rbind(
        res$stat_vals,
        res$stat_vals_s
      )
data.frame(model_type = c("full","forward"), stat_vals)
```

Model Statistics in test data
```{r,echo=FALSE}
stat_vals_test <- rbind(
          res$stat_vals_test,
          res$stat_vals_s_test
        )
data.frame(model_type = c("full","forward"), stat_vals_test)
```

Create the scorecard
```{r, message=FALSE, warning=FALSE}
offset <- 600 - (28.8539 *log(50))
factor <- 20/log(2)

eff_model <- res$glm_s
# eff_model <- res$glm

beta <- eff_model$coefficients
      
      
beta0 <- beta["(Intercept)"]

n <- length(beta) - 1
coef_tab <- eff_model$coefficients[-1] %>% data.frame %>% 
  mutate(VARIABLE = gsub("_woe","",names(beta[-1]))) %>% 
  rename(beta = ".")


scorecard_full <- coarse_class_woe_dt %>% inner_join(coef_tab) %>% 
  mutate(score = -(woe*100*beta+beta0/n)*factor+offset/n) %>% 
  inner_join(coarse_class_dt) %>%
  select(VARIABLE,BIN,NEW_LABEL,score) %>% unique


(scorecard <- scorecard_full %>% 
        arrange(VARIABLE,BIN) %>%
        select(VARIABLE,NEW_LABEL,score) %>% unique)
```



Plots
```{r}
plot(res[[paste0("glm_s")]])

print(res[["confusion_matrix_s"]])

perf1 <- performance(res[["pred_s"]],"tpr","fpr")
plot(perf1, main="ROC Curve")

perf2 <- performance(res[["pred_s"]],"lift","rpp")
plot(perf2, main="Lift Curve")

print(res[["confusion_matrix_s_test"]])

perf1 <- performance(res[["pred_s_test"]],"tpr","fpr")
plot(perf1, main="ROC Curve")

perf2 <- performance(res[["pred_s_test"]],"lift","rpp")
plot(perf2, main="Lift Curve")
```

