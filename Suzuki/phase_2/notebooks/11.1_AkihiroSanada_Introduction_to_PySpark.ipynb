{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparkの概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sparkはカリフォルニア大学バークレー校AMPLabで開発された、オープンソースの分散処理フレームワークである。  \n",
    "- Amazon, Microsoft, Alibaba, Baidu, NTT, Uberなど、大データを処理する必要のある企業で広く利用されている。\n",
    "- 現在はDatabricksという企業が開発の中心となっている。オープンソースなので、他にも世界中の多くの人々が開発に携わっており、現在の日本人の中心的な開発者としては、NTTの山室氏や猿田氏がいる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SparkはScala, Java, Python, Rで操作することができる。  \n",
    "    - 今回はPythonで操作する方法をお伝えする。\n",
    "    - Spark version2.0以前は、Pythonで操作するとScala/Javaで操作した場合に比べてかなり実行速度が遅かったが、Spark version2.0以降は、ユーザー定義関数を使わずにDataFrame形式(後述)でデータを扱うのであれば、PythonとScala/Javaとで殆ど実行速度は変わらない。\n",
    "    - PythonでSparkを操作するためのラッパーライブラリは<font color=red>**PySpark**</font>と呼ばれている。\n",
    "- Sparkの利用には、クラスタを管理するクラスタマネージャと、分散ファイルシステムが必要である/font。\n",
    "    - クラスタマネージャとしては、デフォルトのSparkクラスタの他、Hadoop YARNクラスタを利用することも可能である。  \n",
    "    - 分散ファイルシステムとしては、Hadoop分散ファイルシステム、MapRファイルシステム、Cassandra, Amazon S3などに対応している。\n",
    "    - 今回は手軽にSparkを試すため、疑似分散ローカルモードを使用する。擬似的な分散ファイルシステムでローカルPCのファイルを扱い、擬似的なSparkクラスタをローカルPCで走らせるモードである。\n",
    "- AWSでは、AWS EMRでSparkクラスタを立ち上げて、S3にファイルを置くことで、容易にSparkを利用できる。Azureでも同様のサービスがある。\n",
    "    - AWS EMR & S3という組み合わせのほか、Databricks & S3という組み合わせもある。こちらの方がより容易/手軽であるが、その分料金は高い。細かい設定はAWS EMRの方がやりやすい印象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 大規模データを素早く処理したい場合は、クラスタ数を増やすことで対応する。例えばUberは10ペタバイト弱のデータがストレージにあった時点で、日々のデータ処理を10000仮想コアのHadoop Yarnクラスタで行っていたようである。(現在はもっと増えている)[参考記事](https://eng.uber.com/uber-big-data-platform/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- このnotebookでは、Sparkの主要機能のうち、Spark SQL(データ操作)とSpark ML(機械学習)を利用する。\n",
    "    - 他、Sparkの主要機能としては、Spark Streaming(ストリーミング分析/処理)、GraphX(グラフ理論分析)などがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySparkで大体のことができてしまうので、Python(と場合によってはSQL)の知識が一定程度あれば、学習コストはかなり低いと言える。  \n",
    "Hadoop YARNやKafkaなど、周辺のライブラリもあわせて学習する場合は、JavaやScalaの知識が必要となってくる。  \n",
    "(ちなみにScalaも関数型言語とオブジェクト指向言語のハイブリッド言語で、楽しい言語である。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではMacBook上でSpark疑似分散ローカルモード & Pyspark & Jupyter notebook環境を構築する方法を紹介する。  \n",
    "(作成にあたっては[こちら](https://qiita.com/neppysan/items/0fe706f04b001c082d38)を参考にした。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. システム設定 > JavaでJavaコントロールパネルを開き、一般タブの\"Javaについて\"ボタンを押して、java version8以降がインストールされていることを確認。\n",
    "    - まだjavaが入っていない場合や、Java version7以前がインストールされている場合は、[こちら](https://www.java.com/ja/download/)からインストール。\n",
    "    - 再度システム設定から、Javaのversionを確認。\n",
    "    - 将来的にJavaでコーディングもするかも、という場合は、[こちら](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)からJDKもインストールしておくと良い。Java SE Development KitのMacOSXのものをダウンロードして実行。\n",
    "        - インストール後、~/.bash_profileに以下を追加。\n",
    "```sh\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)\n",
    "export PATH=\"$JAVA_HOME/bin:$PATH\"\n",
    "```\n",
    "        - `java -version`とターミナルで打って、バージョン情報が表示されたら成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T04:56:54.506292Z",
     "start_time": "2019-08-16T04:56:54.236245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_181\"\r\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_181-b13)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [こちら](https://spark.apache.org/downloads.html)のページから、Sparkをダウンロード&解凍。\n",
    "    - 最新のversion2.4.3, Pre-build for Apache Hadoop 2.7 or laterをダウンロードして、適当な場所に解凍。眞田はHomeディレクトリに解凍した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T04:56:58.000425Z",
     "start_time": "2019-08-16T04:56:57.871434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE    README.md  \u001b[34mconf\u001b[m\u001b[m       \u001b[34mjars\u001b[m\u001b[m       \u001b[34mlogs\u001b[m\u001b[m       \u001b[34mwork\u001b[m\u001b[m\r\n",
      "NOTICE     RELEASE    \u001b[34mdata\u001b[m\u001b[m       \u001b[34mkubernetes\u001b[m\u001b[m \u001b[34mpython\u001b[m\u001b[m     \u001b[34myarn\u001b[m\u001b[m\r\n",
      "\u001b[34mR\u001b[m\u001b[m          \u001b[34mbin\u001b[m\u001b[m        \u001b[34mexamples\u001b[m\u001b[m   \u001b[34mlicenses\u001b[m\u001b[m   \u001b[34msbin\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/spark-2.4.3-bin-hadoop2.7/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ~/.bash_profileを編集。\n",
    "    - ~/.bash_profileに以下を追記。\n",
    "```bash\n",
    "export PATH=$PATH:$HOME/spark-2.4.3-bin-hadoop2.7/bin\n",
    "export PYSPARK_PYTHON=$HOME/anaconda3/envs/{condaの仮想環境の名前}/bin/python\n",
    "export PYSPARK_DRIVER_PYTHON=$HOME/anaconda3/envs/{condaの仮想環境の名前}/bin/jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook' pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1行目で、先程ダウンロード&解凍したsparkのソフトウェアにパスを通している  \n",
    "2行目で、PySparkで使用するPythonを指定している。  \n",
    "3行目では、PySparkで使用するクライアントを指定している。今回はJupyterを使用する  \n",
    "4行目で、pysparkとコマンドを打った場合に自動でnotebookが起動するように設定している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ターミナルから`pyspark`と打って、jupyter notebookが起動したら成功!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. notebookのホーム画面から、本notebookを開き、以下が動作するか確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T06:48:47.074631Z",
     "start_time": "2019-08-21T06:48:47.061889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mjpas250177-552:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10b24d898>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notebook上でも動作をチェック\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkのWeb UI画面にもアクセス可能かどうか確認。[http://localhost:4040](http://localhost:4040)にアクセス。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UIの見方は、例えば[こちら](https://linux.wwing.net/WordPress/2017/01/05/sparkのuiを調べてみた/)を参照。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrameとしてデータを読み込むことができる。これはSpark SQLの機能の一つである。  \n",
    "名前が同じであるが、pandasのDataFrameとは全く別物なので注意。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csvの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T06:05:06.741919Z",
     "start_time": "2019-08-21T06:05:06.734372Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4c2426503380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ~~~/Job1/内の.csvを全てロード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/Job1/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# ~~~/Job1/内の.csvを全てロード\n",
    "df = spark.read.csv(\"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/Job1/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T06:05:01.104234Z",
     "start_time": "2019-08-21T06:05:01.096688Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4eb62ffc6dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 複数のフォルダの.csvを全てロードする場合は、リストで渡せばOK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df_2 = spark.read.csv([\"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/Job1/*.csv\",\n\u001b[0m\u001b[1;32m      3\u001b[0m                      \"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/Job1 copy/*.csv\"])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# 複数のフォルダの.csvを全てロードする場合は、リストで渡せばOK\n",
    "df_2 = spark.read.csv([\"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/Job1/*.csv\",\n",
    "                     \"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/Job1 copy/*.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T06:05:02.673367Z",
     "start_time": "2019-08-21T06:05:02.665367Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9ed363ee7c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# この書き方もできる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/*/*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# この書き方もできる\n",
    "df_3 = spark.read.csv(\"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/*/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:41:42.917993Z",
     "start_time": "2019-08-17T05:41:42.723907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+-----+--------+\n",
      "|_c0|         _c1|       _c2|  _c3|     _c4|\n",
      "+---+------------+----------+-----+--------+\n",
      "|021|201604200750|  0.186631|12461|     1.0|\n",
      "|021|201604200750|  0.186631|12463|     0.0|\n",
      "|021|201604200750|  0.186631|12464|     0.0|\n",
      "|021|201604200750|  0.186631|12467|     1.0|\n",
      "|021|201604200750|  0.186631|12457|5.490196|\n",
      "+---+------------+----------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5) # show(行数)でいい感じに整形して表示してくれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、データ型はsparkが裏側で勝手に推定してくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:41:44.300880Z",
     "start_time": "2019-08-17T05:41:44.266557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes # 全て文字列型で格納されている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csvの他、jsonファイル、parquetファイル、textファイル、hiveテーブル、データベーステーブルからデータを読み込むことができる。  \n",
    "詳細は[公式ドキュメント](https://spark.apache.org/docs/latest/sql-data-sources.html)を参照。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、アクセス先のs3のアクセスキーと秘密キーの設定をしてあげれば、以下のような形で簡単にs3上のファイルもロードできる。\n",
    "```python\n",
    "df = spark.read.csv([\"s3a://example/HDD1/*.csv\", \"s3a://example/HDD2/*.csv\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### レコード数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:42:02.077772Z",
     "start_time": "2019-08-17T05:41:49.128085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89666303"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countで計算できる\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少し時間がかかる。先程の[UIサーバー](http://localhost:4040)で進捗を見ることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列名をつけて読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:42:55.026155Z",
     "start_time": "2019-08-17T05:42:55.008103Z"
    }
   },
   "outputs": [],
   "source": [
    "# toDFメソッドを利用するのがお手軽な方法。複数の引数(カンマ区切りの引数)として入力する必要があるので、\n",
    "# '*'をリストの前につけて、引数展開して入力している\n",
    "df_named = df.toDF(*[\"carid\", \"starttime\", \"times\", \"feature\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:42:56.201622Z",
     "start_time": "2019-08-17T05:42:56.035788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+-------+--------+\n",
      "|carid|   starttime|     times|feature|     val|\n",
      "+-----+------------+----------+-------+--------+\n",
      "|  021|201604200750|  0.186631|  12461|     1.0|\n",
      "|  021|201604200750|  0.186631|  12463|     0.0|\n",
      "|  021|201604200750|  0.186631|  12464|     0.0|\n",
      "|  021|201604200750|  0.186631|  12467|     1.0|\n",
      "|  021|201604200750|  0.186631|  12457|5.490196|\n",
      "+-----+------------+----------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_named.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:42:58.820479Z",
     "start_time": "2019-08-17T05:42:58.813140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carid', 'string'),\n",
       " ('starttime', 'string'),\n",
       " ('times', 'string'),\n",
       " ('feature', 'string'),\n",
       " ('val', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_named.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全て文字列で入ってしまっているので、スキーマを定義して再度読み込むことにする。スキーマとは、データ名とデータ型がセットになったものである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:00.632460Z",
     "start_time": "2019-08-17T05:43:00.629494Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:03.452630Z",
     "start_time": "2019-08-17T05:43:03.448342Z"
    }
   },
   "outputs": [],
   "source": [
    "# StructFieldインスタンスのリストを作成\n",
    "fields = [StructField(\"carid\", StringType()),\n",
    "          StructField(\"starttime\", StringType()),\n",
    "          StructField(\"times\", FloatType()),\n",
    "          StructField(\"feature\", StringType()),\n",
    "          StructField(\"val\", FloatType())]\n",
    "\n",
    "# 作ったリストをStructTypeコンストラクタに渡すと、スキーマが完成。\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringTypeが文字列型、LongTypeがLong整数型、FloatTypeが浮動小数点型である。  \n",
    "データ型については\n",
    "- 整数系: LongType, IntegerType, ShortType, ByteType, \n",
    "- 実数系: FloatType, DoubleType, DecimalType\n",
    "- 日付系: TimestampType, DateType\n",
    "- 文字列系: StringType\n",
    "- リスト系: ArrayType\n",
    "- バイナリデータ系: BinaryType\n",
    "- 真偽値: BooleanType  \n",
    "\n",
    "などがある。  \n",
    "ArrayTypeは、中身についても再度データ型を指定することになる。結果、入れ子構造にできる。  \n",
    "詳細は[公式ドキュメント](https://spark.apache.org/docs/latest/sql-reference.html#data-types)を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:06.364255Z",
     "start_time": "2019-08-17T05:43:06.359918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FloatType"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# schemaへのデータ型へのアクセスは以下のようにできる\n",
    "schema[\"times\"].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:16.029684Z",
     "start_time": "2019-08-17T05:43:15.950397Z"
    }
   },
   "outputs": [],
   "source": [
    "# 以下のようにしてschemaに従って型を変更(キャスト)できる\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# select(後述)の中に、キャストされた各列のリストを与えている。\n",
    "# castするデータ型は、schemaからアクセスしている。\n",
    "df_named_withSchema = df_named.select([col(c).cast(schema[c].dataType) for c in df_named.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:17.707980Z",
     "start_time": "2019-08-17T05:43:17.569664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+--------+\n",
      "|carid|   starttime|   times|feature|     val|\n",
      "+-----+------------+--------+-------+--------+\n",
      "|  021|201604200750|0.186631|  12461|     1.0|\n",
      "|  021|201604200750|0.186631|  12463|     0.0|\n",
      "|  021|201604200750|0.186631|  12464|     0.0|\n",
      "|  021|201604200750|0.186631|  12467|     1.0|\n",
      "|  021|201604200750|0.186631|  12457|5.490196|\n",
      "+-----+------------+--------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_named_withSchema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:19.246076Z",
     "start_time": "2019-08-17T05:43:19.182539Z"
    }
   },
   "outputs": [],
   "source": [
    "# しかし、ロード時に、schemaも一緒に入力するほうが楽である\n",
    "df_named_withSchema = spark.read.csv(\"../data/raw/CAN/20190605_CAN/103.486_CAR1V21_HDD2/Job1/*.csv\", \n",
    "                   schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:20.311654Z",
     "start_time": "2019-08-17T05:43:20.173618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+--------+\n",
      "|carid|   starttime|   times|feature|     val|\n",
      "+-----+------------+--------+-------+--------+\n",
      "|  021|201604200750|0.186631|  12461|     1.0|\n",
      "|  021|201604200750|0.186631|  12463|     0.0|\n",
      "|  021|201604200750|0.186631|  12464|     0.0|\n",
      "|  021|201604200750|0.186631|  12467|     1.0|\n",
      "|  021|201604200750|0.186631|  12457|5.490196|\n",
      "+-----+------------+--------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_named_withSchema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:21.989460Z",
     "start_time": "2019-08-17T05:43:21.983735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carid', 'string'),\n",
       " ('starttime', 'string'),\n",
       " ('times', 'float'),\n",
       " ('feature', 'string'),\n",
       " ('val', 'float')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_named_withSchema.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:23.430238Z",
     "start_time": "2019-08-17T05:43:23.424367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carid: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- times: string (nullable = true)\n",
      " |-- feature: string (nullable = true)\n",
      " |-- val: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# このような表示もできる。nullableが見える分、こちらの方が良いかもしれない\n",
    "df_named.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandasのDataFrameとのやり取り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:44.160840Z",
     "start_time": "2019-08-17T05:43:40.135024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carid</th>\n",
       "      <th>starttime</th>\n",
       "      <th>times</th>\n",
       "      <th>feature</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>021</td>\n",
       "      <td>201604200750</td>\n",
       "      <td>0.186631</td>\n",
       "      <td>12461</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>021</td>\n",
       "      <td>201604200750</td>\n",
       "      <td>0.186631</td>\n",
       "      <td>12463</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>021</td>\n",
       "      <td>201604200750</td>\n",
       "      <td>0.186631</td>\n",
       "      <td>12464</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>021</td>\n",
       "      <td>201604200750</td>\n",
       "      <td>0.186631</td>\n",
       "      <td>12467</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>021</td>\n",
       "      <td>201604200750</td>\n",
       "      <td>0.186631</td>\n",
       "      <td>12457</td>\n",
       "      <td>5.490196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  carid     starttime     times feature       val\n",
       "0   021  201604200750  0.186631   12461  1.000000\n",
       "1   021  201604200750  0.186631   12463  0.000000\n",
       "2   021  201604200750  0.186631   12464  0.000000\n",
       "3   021  201604200750  0.186631   12467  1.000000\n",
       "4   021  201604200750  0.186631   12457  5.490196"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark DataFrame -> pandas DataFrameは toPandas()でできる。\n",
    "# 大容量のデータを無邪気にそのままtoPandasに入れるとローカルPCのメモリが死亡するので注意。\n",
    "df_pandas = df_named_withSchema.limit(5).toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas DataFrame -> Spark DataFrameは toPandas()でできるが、schemaを適切に設定してあげる必要がある場合もあるので注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:56.936988Z",
     "start_time": "2019-08-17T05:43:54.713167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------------+-------+-----------------+\n",
      "|carid|   starttime|             times|feature|              val|\n",
      "+-----+------------+------------------+-------+-----------------+\n",
      "|  021|201604200750|0.1866309940814972|  12461|              1.0|\n",
      "|  021|201604200750|0.1866309940814972|  12463|              0.0|\n",
      "|  021|201604200750|0.1866309940814972|  12464|              0.0|\n",
      "|  021|201604200750|0.1866309940814972|  12467|              1.0|\n",
      "|  021|201604200750|0.1866309940814972|  12457|5.490196228027344|\n",
      "+-----+------------+------------------+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# そのまま読み込むと、sparkがよしなにデータ型を指定してくれるが、小数は全てDoubleType型になってしまう\n",
    "# 結果、Float型を指定していたtimesとvalは、元の有効桁数以下に、変な数字がついてくる。\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:43:58.660325Z",
     "start_time": "2019-08-17T05:43:58.656110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carid: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- times: double (nullable = true)\n",
      " |-- feature: string (nullable = true)\n",
      " |-- val: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:44:00.948513Z",
     "start_time": "2019-08-17T05:44:00.779351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+--------+\n",
      "|carid|   starttime|   times|feature|     val|\n",
      "+-----+------------+--------+-------+--------+\n",
      "|  021|201604200750|0.186631|  12461|     1.0|\n",
      "|  021|201604200750|0.186631|  12463|     0.0|\n",
      "|  021|201604200750|0.186631|  12464|     0.0|\n",
      "|  021|201604200750|0.186631|  12467|     1.0|\n",
      "|  021|201604200750|0.186631|  12457|5.490196|\n",
      "+-----+------------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# このようにきちんとスキーマを指定してあげて読み込めばOK\n",
    "df_spark = spark.createDataFrame(df_pandas, schema=schema)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:44:02.931091Z",
     "start_time": "2019-08-17T05:44:02.927162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carid: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- times: float (nullable = true)\n",
      " |-- feature: string (nullable = true)\n",
      " |-- val: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameの基本的な操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 表示: show/take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:44:06.676128Z",
     "start_time": "2019-08-17T05:44:06.582436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+--------+\n",
      "|carid|   starttime|   times|feature|     val|\n",
      "+-----+------------+--------+-------+--------+\n",
      "|  021|201604200750|0.186631|  12461|     1.0|\n",
      "|  021|201604200750|0.186631|  12463|     0.0|\n",
      "|  021|201604200750|0.186631|  12464|     0.0|\n",
      "|  021|201604200750|0.186631|  12467|     1.0|\n",
      "|  021|201604200750|0.186631|  12457|5.490196|\n",
      "+-----+------------+--------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .show(x): 上位x件を表示。pandasでいうhead\n",
    "df_named_withSchema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:44:08.410587Z",
     "start_time": "2019-08-17T05:44:08.335588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12461', val=1.0),\n",
       " Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12463', val=0.0),\n",
       " Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12464', val=0.0),\n",
       " Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12467', val=1.0),\n",
       " Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12457', val=5.490196228027344)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .take(x): 同じく上位x件を表示\n",
    "df_named_withSchema.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**takeでdouble?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "残念ながらpandasのtailに相当するコマンドはない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行の選択: filter/where/limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:45:16.810655Z",
     "start_time": "2019-08-17T05:44:30.825116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+--------+\n",
      "|carid|   starttime|   times|feature|     val|\n",
      "+-----+------------+--------+-------+--------+\n",
      "|  021|201604200750|0.186631|  12461|     1.0|\n",
      "|  021|201604200750|0.186631|  12463|     0.0|\n",
      "|  021|201604200750|0.186631|  12464|     0.0|\n",
      "|  021|201604200750|0.186631|  12467|     1.0|\n",
      "|  021|201604200750|0.186631|  12457|5.490196|\n",
      "+-----+------------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit: 上位x件のみにデータを絞る\n",
    "# ...のだが、最後をshowで表示すると何故か全体を走査してしまい、かなり時間がかかる。\n",
    "df_named_withSchema.limit(5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:45:16.902793Z",
     "start_time": "2019-08-17T05:45:16.812710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12461', val=1.0),\n",
       " Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12463', val=0.0),\n",
       " Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12464', val=0.0),\n",
       " Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12467', val=1.0),\n",
       " Row(carid='021', starttime='201604200750', times=0.1866309940814972, feature='12457', val=5.490196228027344)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takeで取ればsortは行われない。\n",
    "df_named_withSchema.limit(5).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:17.905314Z",
     "start_time": "2019-08-17T05:48:17.681626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+\n",
      "|carid|   starttime|   times|feature|val|\n",
      "+-----+------------+--------+-------+---+\n",
      "|  021|201604200750|0.186631|  12461|1.0|\n",
      "|  021|201604200750|0.196709|  12461|2.0|\n",
      "|  021|201604200750|0.206723|  12461|3.0|\n",
      "|  021|201604200750|0.216867|  12461|0.0|\n",
      "|  021|201604200750|0.226624|  12461|1.0|\n",
      "+-----+------------+--------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter: 条件に合うレコードのみ取り出す。whereでも同じ。\n",
    "df_named_withSchema.filter(df_named_withSchema.feature == 12461).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:20.553381Z",
     "start_time": "2019-08-17T05:48:20.417877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+\n",
      "|carid|   starttime|   times|feature|val|\n",
      "+-----+------------+--------+-------+---+\n",
      "|  021|201604200750|0.186631|  12461|1.0|\n",
      "|  021|201604200750|0.196709|  12461|2.0|\n",
      "|  021|201604200750|0.216867|  12461|0.0|\n",
      "|  021|201604200750|0.226624|  12461|1.0|\n",
      "|  021|201604200750|0.236824|  12461|2.0|\n",
      "+-----+------------+--------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 複数条件: and\n",
    "df_named_withSchema.filter((df_named_withSchema.feature == 12461) &\n",
    "                          (df_named_withSchema.val <= 2)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:23.576055Z",
     "start_time": "2019-08-17T05:48:23.459150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+\n",
      "|carid|   starttime|   times|feature|val|\n",
      "+-----+------------+--------+-------+---+\n",
      "|  021|201604200750|0.186631|  12461|1.0|\n",
      "|  021|201604200750|0.186631|  12463|0.0|\n",
      "|  021|201604200750|0.186631|  12464|0.0|\n",
      "|  021|201604200750|0.186631|  12467|1.0|\n",
      "|  021|201604200750|0.186631|  12403|1.0|\n",
      "+-----+------------+--------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 複数条件: or\n",
    "df_named_withSchema.filter((df_named_withSchema.feature == 12461) |\n",
    "                          (df_named_withSchema.val <= 2)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:24.064653Z",
     "start_time": "2019-08-17T05:48:23.948183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+--------+\n",
      "|carid|   starttime|   times|feature|     val|\n",
      "+-----+------------+--------+-------+--------+\n",
      "|  021|201604200750|0.186631|  12463|     0.0|\n",
      "|  021|201604200750|0.186631|  12464|     0.0|\n",
      "|  021|201604200750|0.186631|  12467|     1.0|\n",
      "|  021|201604200750|0.186631|  12457|5.490196|\n",
      "|  021|201604200750|0.186631|  12437|   850.0|\n",
      "+-----+------------+--------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not\n",
    "df_named_withSchema.filter(~(df_named_withSchema.feature == 12461)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:24.595605Z",
     "start_time": "2019-08-17T05:48:24.491736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+\n",
      "|carid|   starttime|   times|feature|val|\n",
      "+-----+------------+--------+-------+---+\n",
      "|  021|201604200750|0.186631|  12461|1.0|\n",
      "|  021|201604200750|0.196709|  12461|2.0|\n",
      "|  021|201604200750|0.216867|  12461|0.0|\n",
      "|  021|201604200750|0.226624|  12461|1.0|\n",
      "|  021|201604200750|0.236824|  12461|2.0|\n",
      "+-----+------------+--------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# whereでも同じ\n",
    "df_named_withSchema.where((df_named_withSchema.feature == 12461) &\n",
    "                          (df_named_withSchema.val <= 2)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列の選択: select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:27.480775Z",
     "start_time": "2019-08-17T05:48:27.376509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   times|\n",
      "+--------+\n",
      "|0.186631|\n",
      "|0.186631|\n",
      "|0.186631|\n",
      "+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 単体の列\n",
    "df_named_withSchema.select(\"times\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:30.284505Z",
     "start_time": "2019-08-17T05:48:30.175465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---+\n",
      "|   times|feature|val|\n",
      "+--------+-------+---+\n",
      "|0.186631|  12461|1.0|\n",
      "|0.186631|  12463|0.0|\n",
      "|0.186631|  12464|0.0|\n",
      "+--------+-------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 複数取る場合はリスト\n",
    "df_named_withSchema.select([\"times\", \"feature\", \"val\"]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:30.688192Z",
     "start_time": "2019-08-17T05:48:30.577971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|   times|feature|\n",
      "+--------+-------+\n",
      "|0.186631|  12461|\n",
      "|0.186631|  12463|\n",
      "|0.186631|  12464|\n",
      "+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# または複数の引数で渡す\n",
    "df_named_withSchema.select(\"times\", \"feature\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:32.057828Z",
     "start_time": "2019-08-17T05:48:31.971605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   times|\n",
      "+--------+\n",
      "|0.186631|\n",
      "|0.186631|\n",
      "|0.186631|\n",
      "+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# リストのアクセスとのあわせ技。tで始まるものだけ取り出す。startswithはpythonの文字列のメソッド\n",
    "(df_named_withSchema\n",
    " .select([c for c in df_named_withSchema.columns if c.startswith(\"t\")])\n",
    " .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonｍの文字列のメソッドを忘れてしまった場合は、[標準ドキュメント](https://docs.python.org/ja/3/library/stdtypes.html#str)や[こちらのブログ](http://motw.mods.jp/Python/str_methods.html)などを参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:33.706684Z",
     "start_time": "2019-08-17T05:48:33.609572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|   starttime|   times|\n",
      "+------------+--------+\n",
      "|201604200750|0.186631|\n",
      "|201604200750|0.186631|\n",
      "|201604200750|0.186631|\n",
      "+------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ilocっぽいことを列でやりたい場合\n",
    "df_named_withSchema.select(df_named_withSchema.columns[1:3]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLのselectのようにも使える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:35.197539Z",
     "start_time": "2019-08-17T05:48:35.086825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|(times * 10)|(val + 1)|\n",
      "+------------+---------+\n",
      "|   1.8663099|      2.0|\n",
      "|   1.8663099|      1.0|\n",
      "|   1.8663099|      1.0|\n",
      "+------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_named_withSchema.select(df_named_withSchema.times * 10\n",
    "                           , df_named_withSchema.val + 1).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:37.103301Z",
     "start_time": "2019-08-17T05:48:37.003814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|   sanada| TD|\n",
      "+---------+---+\n",
      "|1.8663099|2.0|\n",
      "|1.8663099|1.0|\n",
      "|1.8663099|1.0|\n",
      "+---------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 列名の変更、SQLでいうところのas ~~は、aliasを使う\n",
    "df_named_withSchema.select((df_named_withSchema.times * 10).alias(\"sanada\")\n",
    "                           , (df_named_withSchema.val + 1).alias(\"TD\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集約関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:39.793449Z",
     "start_time": "2019-08-17T05:48:39.790686Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agg内でfuncを使用することで、集約できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:40.791110Z",
     "start_time": "2019-08-17T05:48:40.742505Z"
    }
   },
   "outputs": [],
   "source": [
    "# さっと計算できるように小さめのデータに\n",
    "df_mini = spark.read.csv(\"../data/raw/CAN/20190605_CAN/\" + \n",
    "                         \"103.486_CAR1V21_HDD2/Job1/\" + \n",
    "                         \"SUZUKI_YSB-021_MRR_2016-04-20_06-27_0001.BLF.csv\",\n",
    "                        schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:43.517474Z",
     "start_time": "2019-08-17T05:48:42.150177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9835622"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mini.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:47.559698Z",
     "start_time": "2019-08-17T05:48:44.271448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(times)|\n",
      "+-----------------+\n",
      "|299.7804090060245|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 平均: mean\n",
    "df_mini.agg(func.mean(\"times\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:51.792580Z",
     "start_time": "2019-08-17T05:48:48.368060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             mean|\n",
      "+-----------------+\n",
      "|299.7804090060245|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agg内で、aliasで名前を変更できる。\n",
    "df_mini.agg(func.mean(\"times\").alias(\"mean\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:48:56.138271Z",
     "start_time": "2019-08-17T05:48:52.756040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------+--------------------+\n",
      "|                  sd|    max(val)|min(val)|            sum(val)|\n",
      "+--------------------+------------+--------+--------------------+\n",
      "|1.471077937646767...|8.4318644E18|-40960.0|2.525231049090134E22|\n",
      "+--------------------+------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 複数の関数で集約する場合\n",
    "df_mini.agg(func.stddev(\"val\").alias(\"sd\"), func.max(\"val\"), func.min(\"val\"),\n",
    "           func.sum(\"val\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:49:04.513147Z",
     "start_time": "2019-08-17T05:48:58.060240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+\n",
      "|feature| sd|max|min|\n",
      "+-------+---+---+---+\n",
      "|  3C307|0.0|0.0|0.0|\n",
      "|  3C134|0.0|0.0|0.0|\n",
      "|  31067|0.0|0.0|0.0|\n",
      "|  12204|0.0|0.0|0.0|\n",
      "|  31477|0.0|0.0|0.0|\n",
      "+-------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupByと組み合わせることができる\n",
    "(df_mini.groupBy(\"feature\").agg(\n",
    "    func.stddev(\"val\").alias(\"sd\")\n",
    "    ,func.max(\"val\").alias(\"max\")\n",
    "    ,func.min(\"val\").alias(\"min\")).show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:49:26.678656Z",
     "start_time": "2019-08-17T05:49:06.040847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+---------------+-----+-----+------------------+----------+-------+\n",
      "|   starttime|feature|             sd|  max|  min|               avg|count(val)|nunique|\n",
      "+------------+-------+---------------+-----+-----+------------------+----------+-------+\n",
      "|201604200627|  3C116|            0.0|  0.0|  0.0|               0.0|      5993|      1|\n",
      "|201604200627|  3D505|            0.0|  0.0|  0.0|               0.0|      5988|      1|\n",
      "|201604200627|  2A205|            0.0|  0.0|  0.0|               0.0|     11979|      1|\n",
      "|201604200627|  3A766|            0.0|  1.0|  1.0|               1.0|      5991|      1|\n",
      "|201604200627|  3BA43|58.544279282335|409.5|205.1|223.52715079127648|      5990|      3|\n",
      "+------------+-------+---------------+-----+-----+------------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 複数のgroupby\n",
    "(df_mini.groupBy(\"starttime\", \"feature\")\n",
    " .agg(\n",
    "    func.stddev(\"val\").alias(\"sd\")\n",
    "    ,func.max(\"val\").alias(\"max\")\n",
    "    ,func.min(\"val\").alias(\"min\")\n",
    "    ,func.mean(\"val\").alias(\"avg\")\n",
    "    ,func.count(\"val\")\n",
    "    ,func.countDistinct(\"val\").alias(\"nunique\")\n",
    " ).show(5))\n",
    "# countDistinctは一つの関数なので注意。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, mean, countなど、いくつかの関数はメソッドとしても実装されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:49:43.407010Z",
     "start_time": "2019-08-17T05:49:37.791449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+\n",
      "|   starttime|feature|count|\n",
      "+------------+-------+-----+\n",
      "|201604200627|  3C116| 5993|\n",
      "|201604200627|  3A766| 5991|\n",
      "|201604200627|  3D505| 5988|\n",
      "|201604200627|  2A205|11979|\n",
      "|201604200627|  3BA43| 5990|\n",
      "+------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mini.groupBy(\"starttime\",\"feature\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T05:49:51.150802Z",
     "start_time": "2019-08-17T05:49:44.332644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------------+\n",
      "|   starttime|feature|          avg(val)|\n",
      "+------------+-------+------------------+\n",
      "|201604200627|  3C116|               0.0|\n",
      "|201604200627|  3A766|               1.0|\n",
      "|201604200627|  3D505|               0.0|\n",
      "|201604200627|  2A205|               0.0|\n",
      "|201604200627|  3BA43|223.52715079127648|\n",
      "+------------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mini.groupBy(\"starttime\",\"feature\").mean(\"val\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ウィンドウ関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T06:23:23.155759Z",
     "start_time": "2019-08-17T06:23:23.152673Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T06:31:41.608632Z",
     "start_time": "2019-08-17T06:31:41.570275Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = spark.createDataFrame([(\"apple\", 1), (\"apple\", 2), (\"apple\", 2), (\"apple\", 3),\n",
    "                                 (\"banana\", 4), (\"banana\", 4), (\"banana\", 6), (\"banana\", 7)],\n",
    "                                [\"feature\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T06:32:46.352179Z",
     "start_time": "2019-08-17T06:32:45.733193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+----+----------+------------------+--------+\n",
      "|feature|value|row_number|rank|dense_rank|      percent_rank|quantile|\n",
      "+-------+-----+----------+----+----------+------------------+--------+\n",
      "|  apple|    1|         1|   1|         1|               0.0|       1|\n",
      "|  apple|    2|         2|   2|         2|0.3333333333333333|       2|\n",
      "|  apple|    2|         3|   2|         2|0.3333333333333333|       3|\n",
      "|  apple|    3|         4|   4|         3|               1.0|       4|\n",
      "| banana|    4|         1|   1|         1|               0.0|       1|\n",
      "| banana|    4|         2|   1|         1|               0.0|       2|\n",
      "| banana|    6|         3|   3|         2|0.6666666666666666|       3|\n",
      "| banana|    7|         4|   4|         3|               1.0|       4|\n",
      "+-------+-----+----------+----+----------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(\n",
    "    \"feature\", \"value\"\n",
    "    ,func.row_number().over(Window.partitionBy(\"feature\").orderBy(\"value\")).alias(\"row_number\")\n",
    "    ,func.rank().over(Window.partitionBy(\"feature\").orderBy(\"value\")).alias(\"rank\")\n",
    "    ,func.dense_rank().over(Window.partitionBy(\"feature\").orderBy(\"value\")).alias(\"dense_rank\")\n",
    "    ,func.percent_rank().over(Window.partitionBy(\"feature\").orderBy(\"value\")).alias(\"percent_rank\")\n",
    "    ,func.ntile(4).over(Window.partitionBy(\"feature\").orderBy(\"value\")).alias(\"quantile\")\n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他、lag/lead系とsum(累積和)がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T06:00:32.907403Z",
     "start_time": "2019-08-17T06:00:27.932793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+\n",
      "|carid|   starttime|   times|feature|val|\n",
      "+-----+------------+--------+-------+---+\n",
      "|  021|201604200627|0.295909|  0FA45|0.0|\n",
      "|  021|201604200627|0.295909|  0FA43|0.0|\n",
      "|  021|201604200627|0.295909|  0FA44|0.0|\n",
      "|  021|201604200627|0.295909|  0FA53|0.0|\n",
      "|  021|201604200627|0.295909|  0FA57|4.0|\n",
      "+-----+------------+--------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mini.sort(\"times\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T06:02:13.659764Z",
     "start_time": "2019-08-17T06:02:08.518318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+-------+---+\n",
      "|carid|   starttime|    times|feature|val|\n",
      "+-----+------------+---------+-------+---+\n",
      "|  021|201604200627|599.23865|  3C100|1.0|\n",
      "|  021|201604200627|599.23865|  3C105|0.0|\n",
      "|  021|201604200627|599.23865|  3C101|0.0|\n",
      "|  021|201604200627|599.23865|  3C102|0.0|\n",
      "|  021|201604200627|599.23865|  3C103|0.0|\n",
      "+-----+------------+---------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 逆順にsortするにはfunc.descを使用\n",
    "df_mini.sort(func.desc(\"times\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T06:03:54.370355Z",
     "start_time": "2019-08-17T06:03:49.345566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+-------+---+\n",
      "|carid|   starttime|    times|feature|val|\n",
      "+-----+------------+---------+-------+---+\n",
      "|  021|201604200627| 599.2347|  0FA21|2.0|\n",
      "|  021|201604200627| 599.2247|  0FA21|2.0|\n",
      "|  021|201604200627| 599.2148|  0FA21|2.0|\n",
      "|  021|201604200627|599.20483|  0FA21|2.0|\n",
      "|  021|201604200627| 599.1948|  0FA21|2.0|\n",
      "+-----+------------+---------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 複数キーでsortするにはリストを使って引数に渡すか、複数の引数で渡す。ソート順序はSQLと同様。\n",
    "df_mini.sort(\"feature\", func.desc(\"times\")).show(5)\n",
    "# df_mini.sort([\"feature\", func.desc(\"times\")]).show(5)でもOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 列の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T06:06:48.603369Z",
     "start_time": "2019-08-17T06:06:48.510653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+------+\n",
      "|carid|   starttime|   times|feature|val|newcol|\n",
      "+-----+------------+--------+-------+---+------+\n",
      "|  021|201604200627|0.295909|  0FA53|0.0|     0|\n",
      "|  021|201604200627|0.295909|  0FA57|4.0|     1|\n",
      "|  021|201604200627|0.295909|  0FA43|0.0|     2|\n",
      "|  021|201604200627|0.295909|  0FA44|0.0|     3|\n",
      "|  021|201604200627|0.295909|  0FA45|0.0|     4|\n",
      "+-----+------------+--------+-------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selectで追加する方法\n",
    "df_mini.select(\"carid\", \"satrtime\",\n",
    "               func.monotonically_increasing_id().alias(\"newcol\")).show(5)\n",
    "# 0始まりの昇順IDを付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T05:20:04.276063Z",
     "start_time": "2019-08-16T05:20:04.172605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+-----+\n",
      "|carid|   starttime|   times|feature|val|newID|\n",
      "+-----+------------+--------+-------+---+-----+\n",
      "|  021|201604200627|0.295909|  0FA53|0.0|    0|\n",
      "|  021|201604200627|0.295909|  0FA57|4.0|    1|\n",
      "|  021|201604200627|0.295909|  0FA43|0.0|    2|\n",
      "|  021|201604200627|0.295909|  0FA44|0.0|    3|\n",
      "|  021|201604200627|0.295909|  0FA45|0.0|    4|\n",
      "+-----+------------+--------+-------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnで追加する方法\n",
    "(df_mini.withColumn(\"newID\", func.monotonically_increasing_id()).show(5) # 0始まりの昇順IDを付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T06:07:53.694186Z",
     "start_time": "2019-08-17T06:07:53.608868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+\n",
      "|carid|   starttime|   times|feature|val|\n",
      "+-----+------------+--------+-------+---+\n",
      "|  021|201604200627|0.295909|  0FA53|0.0|\n",
      "|  021|201604200627|0.295909|  0FA57|8.0|\n",
      "|  021|201604200627|0.295909|  0FA43|0.0|\n",
      "|  021|201604200627|0.295909|  0FA44|0.0|\n",
      "|  021|201604200627|0.295909|  0FA45|0.0|\n",
      "+-----+------------+--------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnで既にある列目を指定すると、上書きできる。\n",
    "df_mini.withColumn(\"val\", df_mini.val * 2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLクエリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あたかもSQLのように、データを処理することも可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T05:02:14.663607Z",
     "start_time": "2019-08-16T05:02:14.480656Z"
    }
   },
   "outputs": [],
   "source": [
    "# まず、DataFrameオブジェクトをTemporaryViewに登録する。\n",
    "# これは今起動しているPySparkがシャットダウンされる際に、一緒に削除される\n",
    "df_mini.createOrReplaceTempView(\"df_mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T05:02:34.554965Z",
     "start_time": "2019-08-16T05:02:34.448839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+\n",
      "|carid|   starttime|   times|feature|val|\n",
      "+-----+------------+--------+-------+---+\n",
      "|  021|201604200627|0.295909|  0FA53|0.0|\n",
      "|  021|201604200627|0.295909|  0FA57|4.0|\n",
      "|  021|201604200627|0.295909|  0FA43|0.0|\n",
      "|  021|201604200627|0.295909|  0FA44|0.0|\n",
      "|  021|201604200627|0.295909|  0FA45|0.0|\n",
      "+-----+------------+--------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# このようにしてクエリで呼び出せる。終わりにセミコロン;をつけるとエラーになるので注意。\n",
    "spark.sql(\"select * from df_mini\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T14:08:35.507397Z",
     "start_time": "2019-08-15T14:08:26.230865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+-------+\n",
      "|carid|   starttime|   times|feature|val|lag_val|\n",
      "+-----+------------+--------+-------+---+-------+\n",
      "|  021|201604200627|0.304361|  1EF61|0.0|   null|\n",
      "|  021|201604200627|0.324348|  1EF61|0.0|    0.0|\n",
      "|  021|201604200627|0.344353|  1EF61|0.0|    0.0|\n",
      "|  021|201604200627|0.364349|  1EF61|0.0|    0.0|\n",
      "|  021|201604200627|0.384346|  1EF61|0.0|    0.0|\n",
      "+-----+------------+--------+-------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# window関数にも対応。\n",
    "spark.sql(\"\"\"select *, \n",
    "    lag(val, 1) over(partition by carid, starttime, feature order by times) as lag_val\n",
    "          from df_mini \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T14:17:31.797346Z",
     "start_time": "2019-08-15T14:17:22.277526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------+-------+---+-------+\n",
      "|carid|   starttime|   times|feature|val|lag_val|\n",
      "+-----+------------+--------+-------+---+-------+\n",
      "|  021|201604200627|0.304361|  1EF61|0.0|    0.0|\n",
      "|  021|201604200627|0.324348|  1EF61|0.0|    0.0|\n",
      "|  021|201604200627|0.344353|  1EF61|0.0|    0.0|\n",
      "|  021|201604200627|0.364349|  1EF61|0.0|    0.0|\n",
      "|  021|201604200627|0.384346|  1EF61|0.0|    0.0|\n",
      "+-----+------------+--------+-------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# window関数にも対応。\n",
    "spark.sql(\"\"\"select *, sum(val) over(partition by carid, starttime, feature order by times) as lag_val\n",
    "          from df_mini \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他、以下のようなwindow関数に対応\n",
    "- avg, \n",
    "- ランキング系\n",
    "    - rank\n",
    "    - parcent_rank\n",
    "    - row_number\n",
    "    - cume_dist\n",
    "    - ntile\n",
    "- lag/lead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:suzuki] *",
   "language": "python",
   "name": "conda-env-suzuki-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
